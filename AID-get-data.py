#!/usr/bin/env python3
# Step 1: Import the modules.
from os.path import basename
import sys
from io import StringIO
import json
# import code
# import pprint
import requests
import pandas as pd
from bs4 import BeautifulSoup


_write_csv = False
AID_TARGET_URL = 'https://www.eftlab.com/knowledge-base/complete-list-of-application-identifiers-aid'
RID_TARGET_URL = 'https://www.eftlab.com/knowledge-base/complete-list-of-registered-application-provider-identifiers-rid'
OUT_FILE = 'AID_data.py'


def fetch_tables(url):
    print("fetching", url)
    try:
        # Step 2: Send a request to the Wikipedia page.
        response = requests.get(url, timeout=6)
        response.raise_for_status()  # Check if the request was successful.
    except requests.exceptions.RequestException as e:
        print(f"Error fetching the webpage: {e}")
        return None

    try:
        soup = BeautifulSoup(response.content, "html.parser")

        # <div class="w-embed"><table summary="List of known,,,
        # tables = soup.find_all('div.w-embed')
        # tables = soup.find_all("div", {"class": "w-embed"})
        tables = soup.find_all("table")

        # code.interact(local=dict(globals(), **locals()))

        # Step 3.3: Read the table into a DataFrame using pandas.
        dfs = pd.read_html(StringIO(str(tables)))
    except Exception as e:
        print(f"Error parsing the HTML or reading the table: {e}")
        return None

    return dfs


#code.interact(local=dict(globals(), **locals()))
#sys.exit(0)
if __name__ == '__main__':

    aid_dict ={}

    # Run the function and get the DataFrame.
    df_dat = fetch_tables(AID_TARGET_URL)

    if df_dat is None:
        print("Failed to extract AID tables.")
        sys.exit(0)

    aid_df = pd.concat(df_dat, ignore_index=True) # sort=True
    aid_df.fillna("", inplace=True)
    if _write_csv:
        aid_df.to_csv("AID-data.csv", index=False, sep='\t')

    for x in aid_df.values.tolist():
        aid_dict[x.pop(0)] = x
        # i = x.pop(0)
        # aid_dict[i] = x
        # print(len(x), i)

    hid_dict = {}
    for k, v in aid_dict.items():
        rid = k[:10]
        if rid not in aid_dict and rid not in hid_dict:
            a = [""] * 6
            a[0] = v[0] or v[2] or v[3]  # Vendor or Name or Description
            a[1] = v[1] # Country
            hid_dict[rid] = a

    # aid_dict.update(temp_dict)

    df_dat = fetch_tables(RID_TARGET_URL)

    if df_dat is None:
        print("Failed to extract RID tables.")
        sys.exit(0)

    rid_df = pd.concat(df_dat, ignore_index=True) # sort=True
    rid_df.fillna("", inplace=True)
    if _write_csv:
        rid_df.to_csv("RID-data.csv", index=False, sep='\t')

    for x in rid_df.values.tolist():
        k = x.pop(0)
        if k not in hid_dict:
            a = [""] * 6
            if x[0] or x[1]:
                a[0] = x[0]
                a[1] = x[1]
            else:
                a[0] = a[1] = "???"
            hid_dict[k] = a

    print("AID list size:", len(aid_dict))
    print("HID list size:", len(hid_dict))
    print(f"writing {OUT_FILE}")
    with open(OUT_FILE, 'w', encoding='utf-8') as fd:
        print(f"# Do Not Edit, this file is generated by {basename(__file__)}", file=fd)
        print(f"# Data Downloaded (aka: scrapped) from {AID_TARGET_URL}", file=fd)
        print("# Fields:  AID (Application Identifier): Vendor/Provider, Country, Name, Description, Type", file=fd)
        fd.write('AID_DAT = ')
        json.dump(aid_dict, fd, indent=2, separators=(',', ': '), sort_keys=True)
        fd.write('\n\nHID_DAT = ')
        json.dump(hid_dict, fd, indent=2, separators=(',', ': '), sort_keys=True)
        # pprint.pprint(d, stream=fd, width=200)
